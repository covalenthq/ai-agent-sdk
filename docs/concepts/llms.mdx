---
title: LLMs
description: Overview of supported language models, including OpenAI, Anthropic, and Google Gemini, Grok, and more.
icon: "computer-classic"
---

Each LLM is an adapter around a language model provider and the specific model version, eg: `gpt-4o-mini`. Each [Agent](./agents) can pick their own model and a [ZeeWorkflow](./zeeworkflows) can be configured to use a specific LLM as default.

```tsx
const llm = new LLM({
    provider: "OPEN_AI",
    name: "gpt-4o-mini",
    apiKey: process.env.OPENAI_API_KEY,
    temperature: 0.7
});
```

## List of supported LLMs

### Open AI

<CodeGroup>

```plaintext OpenAI
"gpt-4"
"gpt-4-turbo"
"gpt-3.5-turbo"
"gpt-4o"
"gpt-4o-mini"
"o3-mini"
```

```plaintext DeepSeek
"deepseek-chat"
"deepseek-coder"
```

```plaintext Grok
"grok-2-latest"
"grok-beta"
```

```plaintext Gemini
"gemini-1.5-flash"
"gemini-1.5-pro"
```

```plaintext Gradio
"Any Supported and hosted inference and generation algorithms using gradio python library"
```

</CodeGroup>

## Environment Variables

<CodeGroup>

```plaintext OpenAI
OPENAI_API_KEY
```

```plaintext DeepSeek
DEEPSEEK_API_KEY
```

```plaintext Grok
GROK_API_KEY
```

```plaintext Gemini
GEMINI_API_KEY
```

</CodeGroup>

## Example Usage:

For deepseek-ai/deepseek-vl2-small:

```typescript
const gradioConfig: GradioConfig = {
    provider: "GRADIO",
    name: "deepseek-vl2-small",
    appUrl: "deepseek-ai/deepseek-vl2-small",
    parameters: [
        [["Hello!", null]],
        0.9, // topP
        0.1, // temperature
        0, // repetitionPenalty
        100, // maxGenerationTokens
        0, // maxHistoryTokens
        "deepseek-ai/deepseek-vl2-small", // model name
    ],
};

const llm = new LLM(gradioConfig);
const response = await llm.generate(
    [{ role: "user", content: "Hello!" }],
    { text: z.string() },
    {}
);
console.log(response.value);
```
For r3gm/RVC_HF:
```typescript
const gradioConfig: GradioConfig = {
    provider: "GRADIO",
    name: "RVC_HF",
    appUrl: "r3gm/RVC_HF",
    endpoint: "/run_infer_script",
    parameters: {
        f0up_key: 0,
        filter_radius: 5,
        index_rate: 0,
        rms_mix_rate: 0,
        protect: 0,
        hop_length: 1,
        f0method: "rmvpe",
        input_path: "https://raw.githubusercontent.com/Philotheephilix/Tunify/main/public/amidreaming.wav",
        output_path: "/home/user/app/assets/audios/output.wav",
        pth_path: "logs/Justin_Bieber_v2/Justin_Bieber_v2.pth",
        index_path: "logs/Justin_Bieber_v2/added_IVF1005_Flat_nprobe_1_Justin_Bieber_v2_v2.index",
        clean_strength: 0,
        export_format: "WAV",
        embedder_model: "hubert",
        embedder_model_custom: "",
        upscale_audio: true,
    },
};
```
Gradio is used for custom made inference and algorithms so the working depends solely on the developer of the Gradio Project and the api configured for it.

### Initialization example with Gradio

const llm = new LLM(gradioConfig);
const response = await llm.generate(
    [], // Messages not used for this model
    { result: z.string() },
    {}
);
console.log(response.value);

## Use Cases

### Image Analysis

LLMs can also process images along with text using image URL messages. Here's an example of analyzing an image using the LLM:

```typescript
const messages = [
    {
        role: "user",
        content: [
            {
                type: "text",
                text: "What's in this image? Analyze the logo and suggest improvements.",
            },
            {
                type: "image_url",
                image_url: {
                    url: "https://example.com/logo.png",
                    detail: "auto",
                },
            },
        ],
    },
];

const schema = {
    analysis: z.object({
        description: z.string(),
        colors: z.array(z.string()),
        text_content: z.string().optional(),
        improvements: z.string().optional(),
    }),
};

const result = await llm.generate(messages, schema, {});
```

The LLM will analyze the image and return a structured response containing the description, colors used, and potential improvements. You can also use base64-encoded images by providing the data URL:

```typescript
const messages = [
    {
        role: "user",
        content: [
            {
                type: "text",
                text: "What's in this image and what color is it?",
            },
            {
                type: "image_url",
                image_url: {
                    url: "data:image/png;base64,..." // Your base64 image data
                    detail: "auto",
                },
            },
        ],
    },
];
```

Note: Image analysis is currently supported by OpenAI models. Some providers like Gemini may have limited or no support for image analysis.
