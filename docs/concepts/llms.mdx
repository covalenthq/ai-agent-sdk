---
title: LLMs
description: Overview of supported language models, including OpenAI, Anthropic, and Google Gemini, Grok, and more.
icon: "computer-classic"
---

Each LLM is an adapter around a language model provider and the specific model version, eg: `gpt-4o-mini`. Each [Agent](./agents) can pick their own model and a [ZeeWorkflow](./zeeworkflows) can be configured to use a specific LLM as default.

```tsx
const llm = new LLM({
    provider: "OPEN_AI",
    name: "gpt-4o-mini",
    apiKey: process.env.OPENAI_API_KEY,
    temperature: 0.7
});
```
```tsx
const hfConfig: HuggingFaceConfig = {
    provider: "HUGGINGFACE",
    name: "sentence-transformers/all-MiniLM-L6-v2",
    apiKey: "hf_xxxxxxxxxxxxxxxxxxxxxxxx"
};

const llm = new LLM(hfConfig);
const response = await llm.generate(messages, schema, tools);
```
## List of supported LLMs

### Open AI

<CodeGroup>

```plaintext OpenAI
"gpt-4"
"gpt-4-turbo"
"gpt-3.5-turbo"
"gpt-4o"
"gpt-4o-mini"
"o3-mini"
```

```plaintext DeepSeek
"deepseek-chat"
"deepseek-coder"
```

```plaintext Grok
"grok-2-latest"
"grok-beta"
```

```plaintext Gemini
"gemini-1.5-flash"
"gemini-1.5-pro"
```

```plaintext Hugging Face
Any model name that you have pulled in Ollama
Examples:
"llama2"
"codellama"
"mistral"
"neural-chat"
"DeepSeek-R1"
"Janus-Pro-7B"
"Kokoro-82M"
"Mistral-Small-24B-Instruct-2501"
And any models available in hugging face inference
```

</CodeGroup>

## Environment Variables

<CodeGroup>

```plaintext OpenAI
OPENAI_API_KEY
```

```plaintext DeepSeek
DEEPSEEK_API_KEY
```

```plaintext Grok
GROK_API_KEY
```

```plaintext Gemini
GEMINI_API_KEY
```

```plaintext Hugging Face Inference
HF_API_KEY
```

</CodeGroup>

## Using Hugging Face Models

To use Hugging Face models with the SDK:

1. **Get Hugging Face Token**
   - Create an account at [huggingface.co](https://huggingface.co)
   - Generate an access token in [Account Settings](https://huggingface.co/settings/tokens)

2. **Configure your agent**:
```typescript
const agent = new Agent({
    name: "HuggingFaceAgent",
    model: {
        provider: "HUGGINGFACE",
        name: "mistralai/Mixtral-8x7B-Instruct-v0.1",  // Any supported HF model
        apiKey: "hf_xxxxxxxxxxxxxxxxxxxxxxxx",  // Your Hugging Face token
        baseURL: "https://api-inference.huggingface.co/models", // optional
    },
    description: "Cloud-hosted HF model integration",
});
```

### Response Format
Responses maintain the same structure as other providers:

1. Complete Response Object:
```json
{
  "agent": "hf-agent",
  "messages": [
    {
      "role": "user",
      "content": "Explain quantum computing briefly"
    },
    {
      "role": "assistant",
      "content": {
        "thinking": "Parsing request... Identifying key concepts... Generating concise explanation...",
        "answer": "Quantum computing uses quantum bits to perform calculations through superposition and entanglement..."
      }
    }
  ],
  "status": "completed",
  "children": []
}
```

2. Assistant Message Content Structure:
```json
{
  "role": "assistant",
  "content": {
    "thinking": "Model's internal processing steps",
    "answer": "Final response to user query",
    "details": {
      "model": "mistralai/Mixtral-8x7B-Instruct-v0.1",
      "inference_time": 1.23
    }
  }
}
```

Additional fields:
- `details`: Contains model-specific metadata
- `inference_time`: Time taken for response generation (seconds)

Access responses using:
```typescript
const lastMessage = result.messages[result.messages.length - 1]?.content;
console.log(lastMessage.content.thinking);  // Model's reasoning steps
console.log(lastMessage.content.answer);    // Final response
console.log(lastMessage.content.details?.inference_time);  // Performance metrics
```

### Special Features
1. **Model Switching**:
```typescript
// Dynamically switch models mid-conversation
agent.setModel({
    provider: "HUGGINGFACE",
    name: "google/flan-t5-xxl",
    apiKey: "hf_xxxxxxxxxxxxxxxxxxxxxxxx"
});
```

2. **Task-Specific Endpoints**:
```typescript
// Directly access specialized endpoints
const response = await agent.execute({
    task: "text-to-image",
    parameters: {
        prompt: "A cyberpunk cat hacker",
        model: "stabilityai/stable-diffusion-xl-base-1.0"
    }
});
```

## Use Cases

### Image Analysis

LLMs can also process images along with text using image URL messages. Here's an example of analyzing an image using the LLM:

```typescript
const messages = [
    {
        role: "user",
        content: [
            {
                type: "text",
                text: "What's in this image? Analyze the logo and suggest improvements.",
            },
            {
                type: "image_url",
                image_url: {
                    url: "https://example.com/logo.png",
                    detail: "auto",
                },
            },
        ],
    },
];

const schema = {
    analysis: z.object({
        description: z.string(),
        colors: z.array(z.string()),
        text_content: z.string().optional(),
        improvements: z.string().optional(),
    }),
};

const result = await llm.generate(messages, schema, {});
```

The LLM will analyze the image and return a structured response containing the description, colors used, and potential improvements. You can also use base64-encoded images by providing the data URL:

```typescript
const messages = [
    {
        role: "user",
        content: [
            {
                type: "text",
                text: "What's in this image and what color is it?",
            },
            {
                type: "image_url",
                image_url: {
                    url: "data:image/png;base64,..." // Your base64 image data
                    detail: "auto",
                },
            },
        ],
    },
];
```

Note: Image analysis is currently supported by OpenAI models. Some providers like Gemini may have limited or no support for image analysis.
